---
layout: post
title: Kubeadm 安装1.9.3集群
subtitle: ""
description: "kubeadm是一个kubernetes官方提供的快速安装和初始化拥有最佳实践（best practice）的kubernetes集群的工具，虽然目前还处于 beta 和 alpha 状态，还>不能用在生产环境，但是我们可以通过学习这种部署方法来体会一些官方推荐的kubernetes最佳实践的设计和思想。"
date: 2018-07-09T11:25:13+08:00
author: "FangXiangMeng"
published: true
tags:
  - Kubernetes
categories: [ Docs ]
---

`kubeadm`是一个kubernetes官方提供的快速安装和初始化拥有最佳实践（best practice）的kubernetes集群的工具，虽然目前还处于 beta 和 alpha 状态，还不能用在生产环境，但是我们可以通过学习这种部署方法来体会一些官方推荐的kubernetes最佳实践的设计和思想。

<!--more-->

`kubeadm`的目标是提供一个最小可用的可以通过Kubernetes一致性测试的集群，所以并不会安装任何除此之外的非必须的addon。

`kubeadm`默认情况下并不会安装一个网络解决方案，所以用kubeadm安装完之后 需要自己来安装一个网络的插件。

##  1、安装


### 1.1、系统环境
- Ubuntu16.04+/Debian 9/CentOS 7/RHEL 7/Other.
- 2GB或者以上的RAM(否则将没有足够空间留给app).
- 2核以上CPU.
- 集群的机器之间必须能通过网络互相通信.
- SWAP必须被关闭，否则kubelet会出错！


### 1.2、安装前配置

关闭selinux
```shell
$ setenforce 0
```


关闭防火墙
```shell
$ systemctl stop firewalld
$ systemctl disable firewalld
```


创建/etc/sysctl.d/k8s.conf文件，添加如下内容：
```shell
$ cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

$ modprobe br_netfilter
$ sysctl -p /etc/sysctl.d/k8s.conf
```


安装docker
```shell
$ sudo yum install -y docker
$ sudo systemctl enable docker && sudo systemctl start docker
```


下载所需镜像(需要科学上网)：
```
gcr.io/google_containers/etcd-amd64                      3.1.11              52920ad46f5b        4 months ago        193.2 MB
gcr.io/google_containers/kubernetes-dashboard-amd64      v1.8.3              0c60bcf89900        4 months ago        102.3 MB
gcr.io/google_containers/kube-apiserver-amd64            v1.9.3              360d55f91cbf        5 months ago        210.5 MB
gcr.io/google_containers/kube-proxy-amd64                v1.9.3              35fdc6da5fd8        5 months ago        109.1 MB
gcr.io/google_containers/kube-controller-manager-amd64   v1.9.3              83dbda6ee810        5 months ago        137.8 MB
gcr.io/google_containers/kube-scheduler-amd64            v1.9.3              d3534b539b76        5 months ago        62.71 MB
quay.io/coreos/flannel                                   v0.10.0-amd64       f0fad859c909        5 months ago        44.58 MB
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64     1.14.8              c2ce1ffb51ed        6 months ago        40.95 MB
gcr.io/google_containers/k8s-dns-sidecar-amd64           1.14.8              6f7f2dc7fab5        6 months ago        42.21 MB
gcr.io/google_containers/k8s-dns-kube-dns-amd64          1.14.8              80cc5ea4b547        6 months ago        50.45 MB
gcr.io/google_containers/pause-amd64                     3.0                 da86e6ba6ca1        6 months ago        742.5 kB
k8s.gcr.io/pause-amd64                                   3.1                 da86e6ba6ca1        6 months ago        742.5 kB
gcr.io/google_containers/heapster-influxdb-amd64         v1.3.3              577260d221db        10 months ago       12.55 MB
gcr.io/google_containers/heapster-grafana-amd64          v4.4.3              8cb3de219af7        10 months ago       151.5 MB
gcr.io/google_containers/heapster-amd64                  v1.4.2              d4e02f5922ca        10 months ago       73.4 MB
```


接下来我们需要安装kubeadm, kubelet和kubectl了，我们需要先加一个repo：
```shell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
```


然后安装(所有节点)：
```shell
$ sudo yum install -y kubelet kubeadm kubectl kubernetes-cni
$ sudo systemctl enable kubelet && sudo systemctl start kubelet
```


使用kubeadm初始化master
安装完所有的依赖之后，我们就可以用kubeadm初始化master了。
我们在初始化的时候指定一下kubernetes版本，并设置一下pod-network-cidr（后面的flannel会用到）：
```shell
[root@caas1 /]# kubeadm init --kubernetes-version=v1.9.3 --pod-network-cidr=10.244.0.0/16 
[init] Using Kubernetes version: v1.9.3
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks.
	[WARNING FileExisting-crictl]: crictl not found in system path
[preflight] Starting the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [caas1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.103]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "scheduler.conf"
[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests".
[init] This might take a minute or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 31.503276 seconds
[uploadconfig]?Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[markmaster] Will mark node caas1 as master by adding a label and a taint
[markmaster] Master caas1 tainted and labelled with key/value: node-role.kubernetes.io/master=""
[bootstraptoken] Using token: 820f6b.4cede061bc4845cf
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: kube-dns
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token 820f6b.4cede061bc4845cf 192.168.1.103:6443 --discovery-token-ca-cert-hash sha256:5aa6357262e9b2f173dc2c2f0dca5f17a1212ad06049a14e8d61ac442130211f
 ```
最下面的这行kubeadm join什么的，就是用来让别的node加入集群的，可以看出非常方便。我们要保存好这一行东西，这是我们之后让node加入集群的凭据，一会儿会用到。

这个时候，我们还不能通过kubectl来控制集群，要让kubectl可用，我们需要做：
```shell
# 对于非root用户
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 对于root用户
$ export KUBECONFIG=/etc/kubernetes/admin.conf
# 也可以直接放到~/.bash_profile
$ echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile
```

查看集群状态
```
[root@caas1 flanneld]# kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
scheduler            Healthy   ok                   
etcd-0               Healthy   {"health": "true"} 
```

接下来要注意，我们必须自己来安装一个network addon。

network addon必须在任何app部署之前安装好。同样的，kube-dns也会在network addon安装好之后才启动。kubeadm只支持CNI-based networks（不支持kubenet）。

比较常见的network addon有：
Calico, Canal, Flannel, Kube-router, Romana, Weave Net等。这里我们使用Flannel。

```shell
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
```

安装完network之后，你可以通过kubectl get pods --all-namespaces来查看kube-dns是否在running来判断network是否安装成功。
```shell
[root@caas1 kubernetes]# kubectl get pod --all-namespaces
NAMESPACE     NAME                            READY     STATUS    RESTARTS   AGE
kube-system   etcd-caas1                      1/1       Running   0          1h
kube-system   kube-apiserver-caas1            1/1       Running   0          1h
kube-system   kube-controller-manager-caas1   1/1       Running   0          1h
kube-system   kube-dns-6f4fd4bdf-gwf68        3/3       Running   0          2m
kube-system   kube-flannel-ds-r8ttz           1/1       Running   0          23m
kube-system   kube-proxy-nkxgc                1/1       Running   0          1h
kube-system   kube-scheduler-caas1            1/1       Running   0          1h
```

默认情况下，为了保证master的安全，master是不会被调度到app的。你可以取消这个限制通过输入：
```shell
$ kubectl taint nodes --all node-role.kubernetes.io/master-
```
终于部署完了我们的master！

### 1.3、加入nodes

现在我们开始加入一些node到我们的集群里面吧！

到我们的node节点上，执行刚才下面给出的那个 kubeadm join的命令（每个人不同）：
```shell
 [root@caas2 rpm]# kubeadm join --token 820f6b.4cede061bc4845cf 192.168.1.103:6443 --discovery-token-ca-cert-hash sha256:5aa6357262e9b2f173dc2c2f0dca5f17a1212ad06049a14e8d61ac442130211f
[preflight] Running pre-flight checks.
	[WARNING FileExisting-crictl]: crictl not found in system path
[preflight] Starting the kubelet service
[discovery] Trying to connect to API Server "192.168.1.103:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.1.103:6443"
[discovery] Requesting info from "https://192.168.1.103:6443" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "192.168.1.103:6443"
[discovery] Successfully established connection with API Server "192.168.1.103:6443"

This node has joined the cluster:
* Certificate signing request was sent to master and a response
  was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
```
 
这时候，我们去master上输入kubectl get nodes查看一下：
```shell
[root@caas1 images]# kubectl get nodes
NAME      STATUS    ROLES     AGE       VERSION
caas1     Ready     master    1h        v1.9.3
caas2     Ready     <none>    6m        v1.9.3
```
### 遇到问题
kubelet:
```shell
kubelet: E0710 10:55:50.829239   63543 summary.go:92] Failed to get system container stats for "/system.slice/docker.service": failed to get cgroup stats for "/system.slice/docker.service": failed to get container info for "/system.slice/docker.service": unknown container "/system.slice/docker.service"
```
解决方法：添加`--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice`
```shell
[root@caas2 /]# vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true"
Environment="KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin"
Environment="KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local"
Environment="KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt"
Environment="KUBELET_CADVISOR_ARGS=--cadvisor-port=0"
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice"
Environment="KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS
```
然后重启
```shell
$ systemctl daemon-reload && systemctl restart kubelet
```

### 清空环境
集群初始化如果遇到问题，可以使用下面的命令进行清理：
```shell
kubeadm reset
ifconfig cni0 down
ip link delete cni0
ifconfig flannel.1 down
ip link delete flannel.1
rm -rf /var/lib/cni/
```
### Pod Network(使用CNI)
如果Node有多个网卡的话，目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上--iface=<iface-name>
```yaml
containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.9.1-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=ens160
```
### 如何从集群中移除Node
```shell
kubectl drain node2 --delete-local-data --force --ignore-daemonsets
kubectl delete node node2
```
